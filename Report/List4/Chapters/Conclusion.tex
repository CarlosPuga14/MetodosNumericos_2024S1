\section{Conclusions} \label{sec:conclusions}
The Conjugate Gradient method is a powerful tool employed to solve large sparse linear systems of equations. However, its convergence rate is directly related to the condition number of the matrix. In this work, the Conjugate Gradient method is applied to solve a $2368 \times 2368$ linear system without any preconditioner, and using as preconditioner the Jacobi and SSOR matrices (see eqs. \eqref{eq:Jacobi} and \eqref{eq:SSOR}).

The results show that the system does not converge to the solution without any preconditioner. For the Jacobi and SSOR preconditioners, the system converges to the solution, but at different rates. For 500 iterations, the Jacobi preconditioner reaches an error of $10^{-3}$, while the SSOR preconditioner reaches machine precision. It is important to take into account that the SSOR preconditioner is more computationally expensive since it requires two semi-iterations to solve the linear system. 

The use of a given preconditioner can be previously determined by analyzing the matrix $A$ and its condition number. If the matrix is ill-conditioned, then a preconditioned method is recommended to improve convergence. Between the Jacobi and SSOR preconditioners, the choice depends on how much the condition number of the matrix is reduced, which can be determined by analyzing the eigenvalues of the new matrix $\hat{A} = M^{-1}AM^{-T}$ (see Eq. \eqref{eq:condition_number}). It is known, however, that the SSOR preconditioner usually converges faster than the Jacobi preconditioner, despite being more computationally expensive.

