\section{Introduction}\label{sec:introduction}
The final list of the course on Numerical Methods in Civil Engineering is divided into three main topics: Initial Value Problem (IVP), Boundary Value Problem (BVP), and Least Square Method. All three topics are better discussed in the following sections. During the resolution of the exercises, the following bibliography is used for further understanding of the topics: \cite{de2000metodos,burden1997numerical}.

\subsection{Initial Value Problem (IVP)}\label{subsec:ivp}
In science and engineering, many problems can be modeled using differential equations. In these problems, the rate of change of one or more variables with respect to another (space or time, for example) is considered. 

In the majority of cases, these differential equations are not easily solved analytically so numerical methods are used to approximate the solution. Initial Value Problems are a type of differential equation where the solution is known at a single point. The solution is then propagated to other points using numerical methods such as the Runge-Kutta method.

Runge-Kutta methods, in general, are a family of numerical methods used to solve ordinary differential equations. The biggest advantage of this approach, when compared to Taylor's methods, is that they do not require the computation of the derivatives of the function, which is computationally expensive. The most common Runge-Kutta method employed is the second- and fourth-order methods, which can be expressed through the Butcher Tableau. 

Let Eq. \eqref{eq:ode} be the ordinary differential equation to be solved 
\begin{equation}
    \frac{dy}{dx} = f(x,y), \quad y(x_0) = y_0,
    \label{eq:ode}
\end{equation}
then, the Runge-Kutta methods take the form of 
\begin{equation}
    y_{n+1} = y_n + h\sum_{i=1}^{s}b_ik_i,
\end{equation}
in which $h$ is the step size, $s$ is the number of stages, $b_i$ are the weights (found in the Butcher Tableau), and $k_i$ are the intermediate values. 

The intermediate values are calculated as
\begin{equation}
    k_i = f(x_n + c_ih, y_n + h\sum_{j=1}^{s}a_{ij}k_j),
\end{equation}
where $c_i$ and $a_{ij}$ are the coefficients of the Butcher Tableau. A given Butcher Tableau is given of the form 
\begin{table}[H]
    \centering
    \begin{tabular}{c|ccccc}
        $c_1$ & $a_{11}$ & $a_{12}$ & $\cdots$ & $a_{1s}$ \\
        $c_2$ & $a_{21}$ & $a_{22}$ & $\cdots$ & $a_{2s}$ \\
        $\vdots$ & $\vdots$ & $\vdots$ & $\ddots$ & $\vdots$ \\
        $c_s$ & $a_{s1}$ & $a_{s2}$ & $\cdots$ & $a_{ss}$ \\
        \hline
        & $b_1$ & $b_2$ & $\cdots$ & $b_s$
    \end{tabular}
\end{table}

Exercise 1 implements a class capable of reading a given Butcher Tableau and performing the Runge-Kutta method to solve the equation
\begin{equation}
    y' = 1 + (x-y)^2, \quad \forall 2 \leq x \leq 3, \quad y(2) = 1,
\end{equation}
using the following Runge-Kutta method:
\begin{table}[H]
    \centering
    \begin{tabular}{c|cccc}
        0   \\
        1/3 & 1/3  \\
        2/3 & -1/3 & 1  \\
        1 & 1 & -1 & 1  \\
        \hline
        & 1/8 & 3/8 & 3/8 & 1/8
    \end{tabular}
\end{table}

\subsection{Boundary Value Problem (BVP)}\label{subsec:bvp}
In the previous section, the concept of Initial Value Problems is discussed. In this type of problem, all the specified conditions are given at a single point, usually at the beginning of the interval. For physical problems that are position-dependent rather than time-dependent, the Boundary Value Problem (BVP) is more appropriate, as the conditions are given at more than one point. 

Among the most common methods for solving Boundary Value Problems are the Finite Difference Method and the Finite Element Method. In this list, the ideas of the Galerkin Method, widely used in the Finite Element Method, are used to solve the following BVP
\begin{equation}
    \begin{cases}
        -\nabla \cdot \nabla u(x, y) = f(x, y) & \text{in } \Omega, \\
        u(x, y) = u_D & \text{on } \partial \Omega_D, \\
        \nabla u(x, y) \cdot \bm{n} = h & \text{on } \partial \Omega_N,
    \end{cases}
\end{equation}
where $\Omega$ is the domain in which the differential equation is defined (in this case the square $[-1, 1] \times [-1, 1]$), $\partial \Omega_D$ is the boundary where the Dirichlet condition is applied (left and bottom), $\partial \Omega_N$ is the boundary where the Neumann condition is applied (right and top), $u_D$ is the Dirichlet condition, $h$ is the Neumann condition, and $f(x, y)$ is the source term such as the analytical solution is 
\begin{equation}
    u_{ex} = (x+1)(x-1)\arctan(x-1)\arctan(y-1).
\end{equation}

The first step to solving the BVP using the Galerkin Method is to find the weak formulation for the problem. In this case, the weak formulation is given by
\begin{equation}
    \int_{\Omega} \nabla u \cdot \nabla v \, dx = \int_{\Omega} fv \, dx + \int_{\partial \Omega_N} hv \, ds, \quad \forall v \in V,
\end{equation}
in which $V$ is the space of test functions and $v$ is the test function. 

The second step, and the Galerkin method itself, is to approximate functions $u$ and $v$ by a finite set of basis functions so that 
\begin{equation}
    u(x, y) \approx u_h = \sum_{i=1}^{n} \alpha_i \phi_i(x, y), \quad v(x, y) \approx \sum_{i=1}^{n} \beta_i \phi_i(x, y).
\end{equation}
where $\alpha_i$ and $\beta_i$ are the coefficients of the approximation, $\phi_i(x, y)$ are the basis functions, and $n$ is the number of basis functions.

During the resolution of the problem, Legendre polynomials are used as the basis functions for the approximation space. The order of the polynomial is set $k=\{1,2,3,4,5\}$ and the results are compared. The weak formulation becomes
\begin{equation}
    \sum_{i=1}^{n} \alpha_i \int_{\Omega} \nabla \phi_i \cdot \nabla \phi_j \, dx = \sum_{i=1}^{n}\left(\int_{\Omega} f\phi_j \, dx + \int_{\partial \Omega_N} h\phi_j \, ds\right), 
\end{equation}

By calling $K$ the stiffness matrix and $F$ the load vector, the system of equations to be solved is given by
\begin{equation}
    K\bm{\alpha} = F,
\end{equation}
where
\begin{equation}
    K_{ij} = \int_{\Omega} \nabla \phi_i \cdot \nabla \phi_j \, dx,
\end{equation}
\begin{equation}
    F_i = \int_{\Omega} f\phi_i \, dx + \int_{\partial \Omega_N} h\phi_i \, ds.
\end{equation}



As a way to compare the approximation with the analytical solution, the $L^2$ norm is used so that the approximation error is calculated by 
\begin{equation}
    \|e\| = \sqrt{\int_{\Omega} (u_{ex} - u_h)^2 \, dx}.
\end{equation}

\subsection{Least Square Method}\label{subsec:least_squares_method}
The Least Square Method is used to approximate a set of data points to a given function. This new function now interpolates nontabulated points so that the error between the data points and the function is minimized. 

The method is based on minimizing the sum of the squares of the residuals, which are the differences between the observed values and the values predicted by the model. Least Squares Methods are widely used in regression analysis, curve fitting, and solving overdetermined systems of equations.

Let $E$ be the error function given by 
\begin{equation}
    E = \sum_{i=1}^{m} (y_i - P_n(x_i))^2,
\end{equation}
where $m$ is the number of data points, $n$ is the degree of the polynomial, $a_j$ are the coefficients of the polynomial, $x_i$ and $y_i$ are the data points, and $P_n(x_i)$ is the polynomial evaluated at $x_i$ given by 
\begin{equation}
    P_n(x) = a_0 + a_1x + a_2x^2 + \ldots + a_nx^n.
\end{equation} 

The error function is minimized by solving the linear system of equations associated with the derivatives of the error function with respect to the coefficients $a_{ij}$ of the polynomial
\begin{equation}
    \frac{\partial E}{\partial a_j} = 0 \rightarrow \sum_{k=0}^n a_k \sum_{i=1}^m x_i^{j+k} = \sum_{i=1}^m y_i x_i^j, \quad \forall j = 0,1,\ldots,n.
    \label{eq:partial_error}
\end{equation}

By solving the system of equations in Eq. \eqref{eq:partial_error} the coefficients of the polynomial are found and the function that best fits the data points is obtained. However, the data points may not be perfectly interpolated by the polynomial. In this case, an exponential function can be used to fit the data points
\begin{equation}
    y_i = bx_i^a.
    \label{eq:exp_fit}
\end{equation}

When the data points are fit by Eq. \eqref{eq:exp_fit}, two approaches can be taken: 1) linearize the equation by taking the logarithm of the data points or 2) use a nonlinear solver to find the coefficients $a$ and $b$ that minimize the error function.

In 1), the following procedure is employed: eq. \eqref{eq:exp_fit} is transformed into
\begin{equation}
    \ln(y_i) = \ln(b) + a\ln(x_i),
\end{equation}
and the error function is minimized by solving the linear system as if it were a first-degree polynomial. Then the coefficients $a$ and $b$ are found by taking the exponential of the coefficients found in the linear system. 

In 2), the error function is given by 
\begin{equation}
    E_{exp} = \sum_{i=1}^{m} (y_i - bx_i^a)^2,
    \label{eq:exp_error}
\end{equation}
and the nonlinear system is formed by the derivatives of Eq. \eqref{eq:exp_error} with respect to the coefficients $a$ and $b$
\begin{equation}
    \frac{\partial E_{exp}}{\partial a} = 2\sum_{i=1}^{m} (y_i - bx_i^a)(-b\ln(x_i)x_i^a) = 0,
    \label{eq:exp_error_a}
\end{equation}
\begin{equation}
    \frac{\partial E_{exp}}{\partial b} = 2\sum_{i=1}^{m} (y_i - bx_i^a)(-x_i^a) = 0.
    \label{eq:exp_error_b}
\end{equation}

Solving the nonlinear system of equations formed by eqs. \eqref{eq:exp_error_a} and \eqref{eq:exp_error_b} leads to the coefficients $a$ and $b$ that minimize the error function. To solve the system, any method can be used, such as the Newton method, the Broyden method, or the Secant method.

It is important to mention that solving the linearization of the exponential function, although computationally less expensive, does not return the Least Squares solution. In other words, the solution of the linearization is expected to be less accurate than the solution of the nonlinear system of equations. 

For exercise 3, a set of data points is given, and a code must be implemented so that the data points are fit by an exponential function (linearized and nonlinearized). The results are then compared and the error between the both approximates is calculated.