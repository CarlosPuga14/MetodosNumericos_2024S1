\section{Introduction} \label{sec:introduction}
In mathematics, physics, and many engineering fields, the need to find the solution of a linear system of equations appears. Given a $N\times N$ linear system of the form of Eq. \eqref{eq:linear_system}
\begin{equation}
    Ax = b,
    \label{eq:linear_system}
\end{equation}
in which the matrix $A$ is the coefficient matrix, and the vector $b$ is the equation's right-hand side, the solution vector $x$ can be easily found by using the inverse of the matrix $A$ as shown in Eq. \eqref{eq:solution}
\begin{equation}
    x = A^{-1}b.
    \label{eq:solution}
\end{equation}

The task of inverting a matrix, however, is computationally expensive, and as the problem grows in size, the computational cost increases exponentially. In this context, the decomposition method arises to ease the computational burden of finding the inverse of a matrix. 

This is justified by the fact that inverting the decomposed form $A=LU$ is computationally cheaper than inverting the original matrix $A$. The final solution is then obtained by solving two systems as shown in Eq. \eqref{eq:decomposed_solution}
\begin{equation}
    \begin{cases}
        Ax = LUx = b, \\
        Ly = b, \\
        Ux = y
    \end{cases}.
    \label{eq:decomposed_solution}
\end{equation}
LU, LDU, LLt (or Cholesky), and LDLt decompositions are examples of this methodology, to cite a few. 

This work will focus on the LU and LDLt methods, but the main idea behind all of them is to decompose the original matrix $A$ into two triangular matrices, $L$ (lower triangular), and $U$ (upper triangular). The $D$ stands for a diagonal matrix and might appear in the LDU and LDLt methods. Finally, for the Cholesky and LDLt decompositions, the upper matrix is the transpose of the lower matrix, $U = L^t$, which is achieved due to the symmetry of the problem.

To decompose a matrix, in the following methodology, the rank one update is employed. The rank one update is a method to update a matrix by adding the outer product between the vectors of the line and column of a given equation to the original submatrix formed by all lines and columns below the assessed equation. Although it may vary from method to method, a simple example of rank one update in LU decomposition follows 
\begin{equation*}
    A_{step1} =
    \begin{bmatrix}
        1 & 2 & 3 \\
        4 & 5 & 6 \\
        7 & 8 & 9
    \end{bmatrix},
\end{equation*}

In the matrix $A_{step1}$, $1x_1 + 2x_2 + 3x_3 = b_1$ is the first equation. To update the matrix, the line and column are divided by the pivot (the term in the diagonal of the equation's line, in this case, $1$), and the outer product between the vectors is calculated
\begin{equation*}
    \{4, 7\}^T \otimes \{2, 3\}  = 
    \begin{bmatrix}
        8 & 12 \\
        14 & 21
    \end{bmatrix},
\end{equation*}
and the result is subtracted from the original matrix
\begin{equation*}
    A_{step2} = 
    \begin{bmatrix}
        1 & 2 & 3 \\
        4 & -3 & -6 \\
        7 & -6 & -12
    \end{bmatrix}.
\end{equation*}

This procedure is repeated until the last equation. Once it is over, the resulting matrix is the decomposed form of the original one. The matrix $L$ is the lower triangle of the decomposed matrix (not considering the diagonal) and the matrix $U$ is the upper triangle of the decomposed matrix (including the diagonal). For the cases in which the diagonal matrix is required, the diagonal of the decomposed matrix is the matrix $D$ and $U$ does not include the diagonal.

One problem that might happen is when the pivot is zero. In these cases, the rank one update can not be executed since the division by zero is undetermined. To overcome this issue, the pivoting procedure is employed. It consists of swapping lines and/or columns to ensure that the pivot is not zero. The pivoting procedure is essential to guarantee the convergence of the decomposition method.

Pivoting can be done in two ways: partial pivoting, in which only lines or columns are swapped and full pivoting, in which both lines and columns are swapped. This work focuses on implementing the full pivoting procedure for both LU and LDLt decompositions.

This work also covers structures to storage matrices. Some forms of computational storage matrices include the full matrix, in which all elements are stored, a symmetric matrix, where only the upper or lower triangle is stored, band matrix, where only the diagonal and few elements close to it are stored (the band is more properly the matrix type rather than the storage method itself), and the sparse matrix, in which only the non-zero elements are stored. 

Elements in a sparse matrix are stored in three vectors, usually called cols, values, and ptr. The cols vector stores which column each element is, the values vector stores the value of each element, and the ptr vector stores the cumulative number of elements per line in the matrix.

Sparse matrices are, allegedly, the most efficient way to store matrices, especially when the system is large and contains relatively low non-zero elements. For this reason, the sparse matrix is used in this work and compared to the full matrix storage method.

In this list, LU and LDLt decompositions are implemented and a sparse matrix class is developed to compare its storage performance. Finally, a method to multiply a matrix by a vector is implemented to test the sparse matrix class and results are compared. The literature used herein can be found in \cite{de2000metodos,golub2013matrix,strang2022introduction,jennings1977matrix}